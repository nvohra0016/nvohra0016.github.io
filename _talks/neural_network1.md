---
title: "Neural Networks I: First Network from Scratch"
collection: talks
excerpt: "We describe the framework of neural networks and build an example from scratch using NumPy."
date: 2025-11-15
---

<script type="text/javascript" async
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

# 1. Introduction

Neural Networks may have been around since the 1940s, but their importance and popularity has permeated the current era. Although a lot of articles and web pages talk about their applications, here we discuss the building blocks of neural networks. In particular, we are interested in their mathematical framework, and how they can be used in interpolation and forecasting. 

## 1.1. Components of a Neural Network
A basic neural network may be viewed as a black-box solver that can be "trained" to take an input $x$ and returns an output $y$. For example, suppose we have measured the outside temperature at every hour during the day, but are interested in finding the temperature at $1:45$PM, or around noon the next day. A solution to this is to train a neural network on the hourly temperature values that have been measured to predict the value at any time (down to seconds, minutes etc.). 

A basic neural network has an architecture similar to that shown in Fig. 1. Here we consider a network that takes $x \in \mathbb{R}$ as an input and gives $y \in \mathbb{R}$ as an output. The hidden layer consists of weights and biases $w_{1,i}, b_{1,i}, w_{2,i} \in \mathbb{R}, \; 1 \leq i \leq 4$, and $b_2 \in \mathbb{R}$, along with a nonlinear smooth activation function $\sigma : \mathbb{R} \rightarrow \mathbb{R}$.

<div align="center">
<img src='/images/neural_network1_blog/neural_network_diagram.png' width='500' height='500'>
</div>

<div align = "center">
 Figure 1. Illustration of a single layer neural network with 4 neurons in the hidden layer. The weights are denoted by $w$ and the biases by $b$.
</div>

<br>

In this case, the output $y$ is given by 

\begin{equation}
\label{eq:nn_ex1}
    y = \sum_{i=1}^{4} \sigma\left( w_{2,i} \sigma\left(x w_{1,j} + b_{1,i} \right) + b_{2} \right).
\end{equation}

Or, more succinctly, using matrix vector notation, we rewrite \ref{eq:nn_ex1} as

\begin{equation}
    y = \sigma \left( {w_2}^T \sigma \left(w_1 x + b_1 \right) + b_2 \right)
\end{equation}

where $w_1 = [w_{1,1} \; w_{1,2} \; w_{1,3} \; w_{1,4} ]^T$ (similar definition for $b_1$ and $w_2$), and $\sigma$ is applied component wise to the vector $w_1 x + b_1$. 

Now let us assume that we are given $N \in \mathbb{Z}$ measurements of $y$, denoted by $\hat{y}_n, \; 1 \leq n \leq N$ for corresponding inputs (known) $x_n$. Consider the following problem statement. 

*Problem statement*. How do we find the parameters $w_1, b_1, w_2$ and $b_2$ such that each $y_n$ obtained using $x_n$ from \ref{eq:nn_ex1} is `sufficiently close' to $\hat{y}_n$? 

## 1.2. Minimizing the Loss

In order to select the appropriate parameters $\mathbf{w} = [{w_1}^T \; {b_1}^T \; {w_2}^T \; {b_2}]^T$ we provide a number to what `sufficiently close' means. To this end, we specify the mean squared error as


\begin{equation}
\label{eq:loss_function}
    E = \frac{1}{N}\sum_{n=1}^{N} E_n, \; E_n = \left(\hat{y}_n - y_n \right)^2.
\end{equation}

We now seek to find the best parameters that minimize the loss function $E$ given by \ref{eq:loss_function}. One of the most popular techniques is the gradient descent algorithm: let $\eta > 0$ be given. Define an iterative process as follows: given $\mathbf{w}^{(j)}$, perform the update

\begin{equation}
\label{eq:gradient_descent}
    \mathbf{w}^{(j+1)} = \mathbf{w}^{(j)} - \alpha \nabla_{\mathbf{w}} E\left(\mathbf{w}^{(j)}\right),
\end{equation}

where $\nabla_{\mathbf{w}} E = \left[ \nabla_{b_1}^T E \; \nabla_{w_1}^T E \; \nabla_{w_2}^T E \; \partial_{b_2} E \right]^T$ denotes the Jacobian of $E$ with respect to $\mathbf{w}$. Under a suitable $\eta > 0$ and initial guess $\mathbf{w}^{(0)}$, the sequence $\\{w^{(j)}\\}$ generated by \ref{eq:gradient_descent} converges to the minimizer of $E$. The parameter $\eta$ is known as the learning rate and is specified apriori.

We are now ready to implement our first neural network. We make use of the popular Python library NumPy. 

# 2. Example: Approximating a Cosine Wave

We consider the measurements of a cosine wave $y = \cos{(2\pi x)}$ at $N = 50$ regularly spaced points $x_n$ as shown in Fig. 2. 

<div align="center">
<img src='/images/neural_network1_blog/data_sine.png' width='450' height='450'>
</div>

<div align = "center">
 Figure 2. Plot showing the measured data $y_n$ and input $x_n$ used to train the neural network. 
</div>

<br>

## Code 

The code used in this post has been written by the author and can be found here:...

## References
[^1]: Christopher M. Bishop, *Pattern Recognition and Machine Learning*, 2006, Springer.